{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rFB0ikQgpSR",
        "outputId": "556caad9-9acc-4fa8-a11a-a7f5dc893b4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igGrdR8hu1Yd"
      },
      "source": [
        "#### 1. GPT-4 Vision API 이해하기\n",
        "GPT-4 Vision API는 이미지 입력과 텍스트 프롬프트를 받아 이미지에 대한 상세한 설명이나 질문에 대한 답변을 생성해 줍니다. 이를 통해 이미지 캡셔닝, 장면 이해 등 다양한 작업에 활용할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trm3sSUtYY5G",
        "outputId": "f79bc089-d62d-4a2e-f3c1-2e68bf12cedf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.52.2)\n",
            "Collecting av\n",
            "  Downloading av-13.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (10.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.6.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Downloading av-13.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.1/33.1 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: av\n",
            "Successfully installed av-13.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install openai av matplotlib pillow tqdm\n",
        "\n",
        "import av\n",
        "import openai\n",
        "import base64\n",
        "import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from PIL import Image\n",
        "import time\n",
        "import base64\n",
        "import re\n",
        "import json\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAX8BcxvYPGQ"
      },
      "outputs": [],
      "source": [
        "# OpenAI API 키 설정\n",
        "OPENAI_KEY = # 실제 API 키로 대체하세요\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LI0licbFg0CV"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybsJqH0ZYg0H"
      },
      "outputs": [],
      "source": [
        "def read_video_pyav(container, frame_indices):\n",
        "    frames = []  # 추출된 프레임을 저장할 리스트 초기화\n",
        "    frame_counter = 0  # 현재 프레임 번호를 추적하기 위한 카운터 초기화\n",
        "    frame_indices_set = set(frame_indices)  # 빠른 검색을 위해 프레임 인덱스 목록을 집합(set)으로 변환\n",
        "\n",
        "    # 비디오 컨테이너로부터 프레임을 순차적으로 디코딩\n",
        "    for frame in container.decode(video=0):\n",
        "        # 현재 프레임 번호가 추출 대상 프레임 인덱스에 포함되어 있는지 확인\n",
        "        if frame_counter in frame_indices_set:\n",
        "            # 프레임을 RGB 포맷의 NumPy 배열로 변환\n",
        "            img = frame.to_rgb().to_ndarray()\n",
        "            # 변환된 이미지를 프레임 리스트에 추가\n",
        "            frames.append(img)\n",
        "            # 모든 대상 프레임을 추출했으면 루프 종료\n",
        "            if len(frames) == len(frame_indices):\n",
        "                break\n",
        "        # 다음 프레임으로 이동하기 위해 카운터 증가\n",
        "        frame_counter += 1\n",
        "\n",
        "    # 추출된 프레임들을 NumPy 배열로 반환\n",
        "    return np.array(frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwpRS9D2miqO"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "system_instruction = \"\"\"\n",
        "Analyze the input online presentation video to evaluate the presenter’s non-verbal behaviors, and provide feedback based on the following five categories. For each category, identify any inappropriate behaviors exhibited by the presenter, describe the strengths and areas for improvement in a specific manner, and include concrete examples whenever possible. Additionally, ensure that feedback for each item is provided in 1-2 lines and include the corresponding problematic behavior keyword if an inappropriate behavior is detected.\n",
        "\n",
        "**Categories and Problematic Behavior Keywords:**\n",
        "\n",
        "1. **Eye Contact**\n",
        "   - **Excessive gaze shifting**: When the presenter frequently looks away from the camera or shifts their gaze too often, it disrupts connection with the audience.\n",
        "   - **Irregular gaze dispersion**: Inconsistent eye contact, where the presenter glances in different directions without a consistent focus, can reduce engagement.\n",
        "\n",
        "2. **Facial Expressions**\n",
        "   - **Expressionless**: A lack of expression throughout the presentation can make it challenging for the audience to stay engaged.\n",
        "   - **Excessive facial changes**: Frequent, abrupt changes in facial expressions can be distracting and reduce the effectiveness of the message.\n",
        "\n",
        "3. **Gestures and Hand Movements**\n",
        "   - **Excessive hand movements**: Constant or overly exaggerated hand movements may distract the audience from the presentation content.\n",
        "   - **Unnecessary hand movements**: Movements not relevant to the content may make the presentation appear less professional.\n",
        "\n",
        "4. **Posture and Body Language**\n",
        "   - **Slouched posture**: Poor posture, such as slouching, can project a lack of confidence.\n",
        "   - **Excessive movement**: Frequent shifting or large body movements may distract the audience and affect the presentation flow.\n",
        "\n",
        "5. **Sudden Actions and Movements**\n",
        "   - **Unexpected actions**: Abrupt actions or gestures without purpose can divert the audience’s attention unexpectedly.\n",
        "   - **Disrupting presentation flow**: Actions that break the flow of the presentation can make it harder for the audience to follow the message.\n",
        "\n",
        "**Feedback Format:**\n",
        "\n",
        "- **Eye Contact**\n",
        "  - **Strengths:** [Specific positive feedback]\n",
        "  - **Areas for Improvement:** [Specific improvement suggestions] [Problematic Behavior Keyword]\n",
        "\n",
        "- **Facial Expressions**\n",
        "  - **Strengths:** [Specific positive feedback]\n",
        "  - **Areas for Improvement:** [Specific improvement suggestions] [Problematic Behavior Keyword]\n",
        "\n",
        "- **Gestures and Hand Movements**\n",
        "  - **Strengths:** [Specific positive feedback]\n",
        "  - **Areas for Improvement:** [Specific improvement suggestions] [Problematic Behavior Keyword]\n",
        "\n",
        "- **Posture and Body Language**\n",
        "  - **Strengths:** [Specific positive feedback]\n",
        "  - **Areas for Improvement:** [Specific improvement suggestions] [Problematic Behavior Keyword]\n",
        "\n",
        "- **Sudden Actions and Movements**\n",
        "  - **Strengths:** [Specific positive feedback]\n",
        "  - **Areas for Improvement:** [Specific improvement suggestions] [Problematic Behavior Keyword]\n",
        "\n",
        "**Additional Guidelines:**\n",
        "- When pointing out inappropriate behaviors, provide specific examples along with suggestions for improvement.\n",
        "- Include the corresponding problematic behavior keyword at the end of the \"Areas for Improvement\" section if an inappropriate behavior is detected.\n",
        "- Balance feedback by highlighting both positive and inappropriate behaviors to maintain the presenter’s motivation.\n",
        "- Explain how the presenter’s behaviors relate to the objectives of the presentation.\n",
        "- Use concrete examples to enhance the clarity of the feedback.\n",
        "\n",
        "**Examples:**\n",
        "\n",
        "- **Eye Contact**\n",
        "  - **Strengths:** \"You maintained steady eye contact with the camera throughout the presentation, effectively engaging the audience.\"\n",
        "  - **Areas for Improvement:** \"You were frequently looking away from the camera during the presentation. Maintaining eye contact with the camera will facilitate better communication.\" [Excessive gaze shifting]\n",
        "\n",
        "- **Facial Expressions**\n",
        "  - **Strengths:** \"You used appropriate facial expressions when explaining key points, which made your message clearer.\"\n",
        "  - **Areas for Improvement:** \"You appeared expressionless throughout the presentation, making it difficult to capture the audience’s interest. Try using varied facial expressions to convey emotions.\" [Excessive facial changes]\n",
        "\"\"\"\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "848OZpHHwwDJ"
      },
      "outputs": [],
      "source": [
        "system_instruction = \"\"\"\n",
        "당신은 15년 이상의 경력을 가진 온라인 발표 전문 코치입니다. 비언어적 커뮤니케이션 분야의 전문가로서, 수많은 발표자들이 비언어적 행동을 개선하도록 도왔습니다. 당신은 발표자의 온라인 발표에서의 제스처, 표정, 시선 처리, 자세 등이 청중에게 미치는 영향을 깊이 이해하고 있으며, 이를 토대로 구체적이고 실용적인 피드백을 제공합니다.\n",
        "입력된 온라인 발표 영상을 분석하여 발표자의 비언어적 행동을 평가하고, 다음 다섯 가지 카테고리를 기준으로 피드백을 제공해주세요. 각 카테고리마다 발표자가 보인 부적절한 행동을 식별하고, 개선이 필요한 점을 구체적으로 서술하며, 가능한 경우 구체적인 예시를 포함해주세요. 피드백은 각 항목당 1~2줄로 간결하게 제공되며, 부적절한 행동이 감지된 경우 해당 문제 행동의 정의 키워드를 포함시켜주세요.\n",
        "\n",
        "추가 지침:\n",
        "\n",
        "- 부적절한 행동을 지적할 때는 구체적인 예시와 함께 개선 방안을 제시해주세요.\n",
        "- 긍정적인 행동은 제외하고, 개선이 필요한 행동에 집중하여 피드백을 제공해주세요.\n",
        "- 가능한 한 구체적인 예시를 들어 피드백의 명확성을 높여주세요.\n",
        "- 하나의 카테고리에서 여러 부적절한 행동이 감지될 경우 모두 언급하여 발표자가 명확하게 이해할 수 있도록 합니다.\n",
        "\n",
        "카테고리 및 부적절한 행동 키워드:\n",
        "\n",
        "1. 시선 처리\n",
        "   - 과도한 시선 이동\n",
        "     - 예시: 발표자가 중요한 내용을 설명할 때마다 자주 화면 밖을 보거나 주변을 둘러보며 시선을 지속적으로 이동합니다.\n",
        "     - 감지 기준:\n",
        "       - 발표자의 눈이 카메라를 바라보지 않고 다른 곳을 응시함.\n",
        "   - 불규칙한 시선 분산\n",
        "     - 예시: 발표 중간중간 시선을 갑자기 왼쪽, 오른쪽, 아래 등 다양한 방향으로 자주 돌려 일관된 시선 유지를 하지 못합니다.\n",
        "     - 감지 기준:\n",
        "       - 시선을 특정 지점에 지속적으로 유지하지 못하고 불규칙하게 움직임.\n",
        "\n",
        "2. 얼굴 표정\n",
        "   - 무표정\n",
        "     - 예시: 발표 내내 무표정을 유지하여 감정을 전달하지 못해 청중의 관심을 끌기 어렵습니다.\n",
        "     - 감지 기준:\n",
        "       - 발표 시간의 80% 이상 동안 얼굴에 변화가 거의 없음.\n",
        "   - 과도한 표정 변화\n",
        "     - 예시: 발표 도중 지나치게 많은 표정 변화를 보여 자연스럽지 않고 산만하게 보입니다.\n",
        "     - 감지 기준:\n",
        "       - 표정이 자주 변하여 청중의 집중을 방해함.\n",
        "\n",
        "3. 제스처 및 손동작\n",
        "   - 과도한 손동작\n",
        "     - 예시: 발표 하는 도중에 갑자기 화면에 손이 나오거나, 손을 흔들거나 움직여 청중의 집중을 방해합니다.\n",
        "     - 감지 기준:\n",
        "       - 발표 내용과 관련 없는 손동작으로 청중의 주의를 분산시킴.\n",
        "   - 불필요한 손동작\n",
        "     - 예시: 발표 내용과 관련 없는 손동작을 반복적으로 사용하여 산만하게 만듭니다. 예를 들어, 설명과 무관하게 손을 계속해서 얼굴 가까이로 가져갑니다.\n",
        "     - 감지 기준:\n",
        "       - 손동작이 발표의 흐름과 무관하게 이루어져 청중의 집중을 방해함.\n",
        "\n",
        "4. 자세 및 신체 언어\n",
        "   - 구부정한 자세\n",
        "     - 예시: 발표자가 자세를 바르게 하지 않고 고개를 숙이면서 청중의 집중을 방해합니다.\n",
        "     - 감지 기준:\n",
        "       - 갑자기 숙임, 화면을 벗어나는등 비전문적인 자세.\n",
        "   - 과도한 움직임\n",
        "     - 예시: 발표 중 자리에서 일어나거나, 몸을 좌우 앞뒤로 크게 움직여 발표의 흐름을 방해합니다.\n",
        "     - 감지 기준:\n",
        "       - 발표 화면내에서 허리와 등을 곧게 펴지 않는 자세\n",
        "       - 발표의 흐름을 방해하고 청중의 집중을 흐트러뜨림.\n",
        "\n",
        "5. 갑작스러운 행동 및 움직임\n",
        "   - 예상치 못한 행동\n",
        "     - 예시: 발표 도중 갑자기 손을 크게 올리거나, 몸을 급격하게 돌리는 등 예측할 수 없는 행동을 합니다. 예를 들어, 중요한 포인트 없이 갑자기 손을 크게 흔듭니다.\n",
        "     - 감지 기준:\n",
        "       - 발표 중 발표화면에서 발표자의 예상치 못한 행동이 발생.\n",
        "       - 청중의 주의를 갑작스럽게 분산시켜 발표의 일관성을 해침.\n",
        "   - 발표 흐름 방해\n",
        "     - 예시: 발표 중 중요한 내용을 설명할 때 갑자기 손을 올려 청중의 주의를 분산시킵니다. 예를 들어, 슬라이드를 설명하는 도중에 불필요하게 손을 크게 움직입니다.\n",
        "     - 감지 기준:\n",
        "       - 발표 화면에서 발표 흐름을 방해하는 행동이 발생.\n",
        "       - 발표의 일관성을 유지하지 못하고 청중의 집중을 분산시킴.\n",
        "\n",
        "피드백 형식:\n",
        "\n",
        "- 시선 처리\n",
        "  - 개선이 필요한 점: \"발표 중간에 자주 화면 밖을 보시는 모습이 관찰되었습니다. 카메라를 향해 시선을 유지하시면 더 효과적인 소통이 가능할 것입니다.\" [과도한 시선 이동]\n",
        "  - 권장 사항: \"카메라와 시선을 고정하여 청중과 일관된 연결을 유지하도록 연습해 보세요.\"\n",
        "\n",
        "- 얼굴 표정\n",
        "  - 개선이 필요한 점: \"발표 내내 무표정하게 보이는 부분이 있어 청중의 관심을 끌기 어려웠습니다. 다양한 표정을 사용하여 감정을 표현해보세요.\" [무표정]\n",
        "  - 권장 사항: \"강조할 때나 중요한 순간에 미소나 표정 변화를 추가하면 더 생동감 있는 발표가 될 것입니다.\"\n",
        "\n",
        "- 제스처 및 손동작\n",
        "  - 개선이 필요한 점: \"발표 도중 손을 너무 많이 흔드셔서 산만하게 보일 수 있었습니다. 주요 포인트에서만 손동작을 사용해보시면 좋겠습니다.\" [과도한 손동작]\n",
        "  - 권장 사항: \"핵심 포인트에만 손동작을 사용하여 발표의 주목도를 높여 보세요.\"\n",
        "\n",
        "- 자세 및 신체 언어\n",
        "  - 개선이 필요한 점: \"발표 중간에 자주 자세가 바뀌는 모습이 보였습니다. 안정된 자세를 유지하시면 더욱 집중된 발표가 될 것입니다.\" [구부정한 자세]\n",
        "  - 권장 사항: \"앉거나 서 있을 때 몸을 일직선으로 유지해 안정감 있는 인상을 주도록 해보세요.\"\n",
        "\n",
        "- 갑작스러운 행동 및 움직임\n",
        "  - 개선이 필요한 점: \"발표 도중 예상치 못한 손동작을 자주 사용하셨습니다. 손동작을 좀 더 계획적으로 사용하시면 좋을 것 같습니다.\" [예상치 못한 행동]\n",
        "  - 권장 사항: \"강조가 필요할 때만 움직임을 추가하여 청중의 집중을 유도하세요.\"\n",
        "\n",
        "\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gx9nQFQBx5mz"
      },
      "outputs": [],
      "source": [
        "# 문제 행동의 정의 키워드 목록\n",
        "PROBLEMATIC_BEHAVIORS = [\n",
        "    \"과도한 시선 이동\",\n",
        "    \"불규칙한 시선 분산\",\n",
        "    \"무표정\",\n",
        "    \"과도한 표정 변화\",\n",
        "    \"과도한 손동작\",\n",
        "    \"불필요한 손동작\",\n",
        "    \"구부정한 자세\",\n",
        "    \"과도한 움직임\",\n",
        "    \"예상치 못한 행동\",\n",
        "    \"발표 흐름 방해\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxtEpuOSwwxX"
      },
      "outputs": [],
      "source": [
        "# 사용자 프롬프트 생성 함수\n",
        "def generate_user_prompt(img_type, img_b64_str):\n",
        "    \"\"\"\n",
        "    주어진 이미지 데이터와 타입을 기반으로 비언어적 행동 평가를 요청하는 사용자 프롬프트를 생성합니다.\n",
        "    \"\"\"\n",
        "    return f\"Please evaluate the presenter's non-verbal behavior.\\n\\nImage data: data:{img_type};base64,{img_b64_str}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbSZ8Ajcw3FR"
      },
      "outputs": [],
      "source": [
        "# 이미지 인코딩 함수\n",
        "def encode_image(image, max_size=(256, 256), quality=70):\n",
        "    \"\"\"\n",
        "    이미지를 리사이즈하고 JPEG 형식으로 인코딩한 후 Base64 문자열로 반환합니다.\n",
        "\n",
        "    Parameters:\n",
        "    - image: PIL.Image 형식의 이미지\n",
        "    - max_size: 이미지의 최대 크기 (기본값: 256x256)\n",
        "    - quality: JPEG 압축 품질 (기본값: 70)\n",
        "\n",
        "    Returns:\n",
        "    - Base64로 인코딩된 이미지 문자열\n",
        "    \"\"\"\n",
        "    # 이미지 리사이즈\n",
        "    image.thumbnail(max_size)\n",
        "    buffered = io.BytesIO()\n",
        "    image.save(buffered, format=\"JPEG\", quality=quality)\n",
        "    img_b64_str = base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
        "    return img_b64_str\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y90p_jf0B1ez"
      },
      "outputs": [],
      "source": [
        "# 비디오 길이 가져오기 함수\n",
        "def get_video_duration(video_path):\n",
        "    \"\"\"\n",
        "    주어진 비디오 파일의 전체 길이(초 단위)를 반환합니다.\n",
        "\n",
        "    Parameters:\n",
        "    - video_path: 비디오 파일 경로\n",
        "\n",
        "    Returns:\n",
        "    - 비디오 길이 (초 단위)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        container = av.open(video_path)\n",
        "        stream = container.streams.video[0]\n",
        "        duration = stream.duration * stream.time_base\n",
        "        container.close()\n",
        "        return duration\n",
        "    except Exception as e:\n",
        "        print(f\"비디오 길이 가져오기 실패: {e}\")\n",
        "        return 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ct-aWeptB3q0"
      },
      "outputs": [],
      "source": [
        "# 비디오에서 프레임 추출 함수\n",
        "def download_and_sample_video_local(video_path, start_time=0, duration=60, frame_interval=3):\n",
        "    \"\"\"\n",
        "    주어진 비디오 파일에서 지정된 시작 시간과 지속 시간 내에서 일정 간격으로 프레임을 추출합니다.\n",
        "\n",
        "    Parameters:\n",
        "    - video_path: 비디오 파일 경로\n",
        "    - start_time: 추출 시작 시간 (초 단위)\n",
        "    - duration: 추출할 구간의 길이 (초 단위)\n",
        "    - frame_interval: 프레임 추출 간격 (초 단위)\n",
        "\n",
        "    Returns:\n",
        "    - 추출된 프레임들의 리스트 (NumPy 배열 형식)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 비디오 파일 열기\n",
        "        container = av.open(video_path)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"비디오 파일을 찾을 수 없습니다: {video_path}\")\n",
        "        return None\n",
        "    except av.AVError as e:\n",
        "        print(f\"비디오 파일을 여는 중 오류가 발생했습니다: {e}\")\n",
        "        return None\n",
        "\n",
        "    stream = container.streams.video[0]\n",
        "    fps = float(stream.average_rate) if stream.average_rate else 30.0\n",
        "\n",
        "    # 세그먼트 내에서 추출할 프레임의 타임스탬프 계산\n",
        "    num_frames = int(duration / frame_interval) + 1\n",
        "    timestamps = [\n",
        "        start_time + i * frame_interval\n",
        "        for i in range(num_frames)\n",
        "        if start_time + i * frame_interval <= start_time + duration\n",
        "    ]\n",
        "\n",
        "    frames = []\n",
        "    index = 0\n",
        "    seek_pts = int(start_time / stream.time_base)\n",
        "    container.seek(seek_pts, any_frame=False, backward=True, stream=stream)\n",
        "\n",
        "    for frame in container.decode(video=0):\n",
        "        frame_time = frame.time\n",
        "        if frame_time is None:\n",
        "            continue\n",
        "        if frame_time < start_time:\n",
        "            continue\n",
        "        if frame_time >= start_time + duration:\n",
        "            break\n",
        "\n",
        "        while index < len(timestamps) and frame_time >= timestamps[index]:\n",
        "            frames.append(frame.to_rgb().to_ndarray())\n",
        "            index += 1\n",
        "            if index >= len(timestamps):\n",
        "                break\n",
        "\n",
        "    container.close()\n",
        "    return np.array(frames)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdPga9vJZiDb"
      },
      "outputs": [],
      "source": [
        "def analyze_frames(frames, segment_idx, duration, segment_length, frame_interval=3):\n",
        "    problematic_frames = []\n",
        "    feedbacks = []\n",
        "\n",
        "    PROBLEMATIC_BEHAVIORS = [\n",
        "        \"과도한 시선 이동\",\n",
        "        \"불규칙한 시선 분산\",\n",
        "        \"무표정\",\n",
        "        \"과도한 표정 변화\",\n",
        "        \"과도한 손동작\",\n",
        "        \"불필요한 손동작\",\n",
        "        \"구부정한 자세\",\n",
        "        \"과도한 움직임\",\n",
        "        \"예상치 못한 행동\",\n",
        "        \"발표 흐름 방해\"\n",
        "    ]\n",
        "\n",
        "    num_frames = len(frames)\n",
        "    time_stamps = [\n",
        "        segment_idx * segment_length + i * frame_interval\n",
        "        for i in range(num_frames)\n",
        "    ]\n",
        "\n",
        "    for i, (frame, frame_time_sec) in enumerate(zip(frames, time_stamps)):\n",
        "        minutes = int(frame_time_sec // 60)\n",
        "        seconds = int(frame_time_sec % 60)\n",
        "        timestamp = f\"{minutes}m {seconds}s\"\n",
        "\n",
        "        # 사용자 프롬프트 생성\n",
        "        user_prompt = (\n",
        "            \"다음 이미지에서 발표자의 비언어적 행동을 분석하고, system_instruction 내용에 기반해서 문제가 되는 행동이 있으면 피드백을 제공해주세요. \"\n",
        "            \"문제가 없으면 '문제 없음'이라고 답해주세요.\"\n",
        "        )\n",
        "\n",
        "        img_type = \"image/jpeg\"\n",
        "\n",
        "        # 이미지를 인코딩\n",
        "        image = Image.fromarray(frame)\n",
        "        img_b64_str = encode_image(image)\n",
        "\n",
        "        # 사용자 메시지 구성\n",
        "        user_message = f\"{user_prompt}\\n\\n이미지 데이터: data:{img_type};base64,{img_b64_str}\"\n",
        "\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=\"gpt-4o\",  # 사용 가능한 모델 이름으로 설정\n",
        "                messages=[\n",
        "                    {\n",
        "                        \"role\": \"system\",\n",
        "                        \"content\": system_instruction\n",
        "                    },\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": user_message\n",
        "                    }\n",
        "                ],\n",
        "                max_tokens=600,\n",
        "            )\n",
        "\n",
        "            # 생성된 텍스트와 문제 행동 추출\n",
        "            generated_text = response.choices[0].message.content\n",
        "            behaviors_detected = re.findall(r'\\[([^\\[\\]]+)\\]', generated_text)\n",
        "\n",
        "            # 공백 제거\n",
        "            behaviors_detected = [behavior.strip() for behavior in behaviors_detected]\n",
        "\n",
        "            # 디버깅을 위해 감지된 문제 행동 출력\n",
        "            print(f\"[디버그] 프레임 {i+1} 응답 텍스트: {generated_text}\")\n",
        "            print(f\"[디버그] 감지된 문제 행동: {behaviors_detected}\")\n",
        "            print(f\"[디버그] PROBLEMATIC_BEHAVIORS 리스트: {PROBLEMATIC_BEHAVIORS}\")\n",
        "\n",
        "            # 문제 행동 감지 여부 확인\n",
        "            problem_detected = any(behavior in PROBLEMATIC_BEHAVIORS for behavior in behaviors_detected)\n",
        "\n",
        "            if problem_detected:\n",
        "                # 프레임과 세그먼트 정보를 저장\n",
        "                problematic_frames.append((frame, segment_idx + 1, i + 1, timestamp))\n",
        "                feedbacks.append(generated_text)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"프레임 {i+1} 처리 중 오류 발생: {e}\")\n",
        "\n",
        "    return problematic_frames, feedbacks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3coqnCjoQC3"
      },
      "outputs": [],
      "source": [
        "def plot_problematic_frames(frames, feedbacks):\n",
        "    if not frames:\n",
        "        print(\"문제 있는 프레임이 없습니다.\")\n",
        "        return\n",
        "\n",
        "    for i, (frame_info, feedback) in enumerate(zip(frames, feedbacks)):\n",
        "        frame, segment_number, frame_number, timestamp = frame_info\n",
        "\n",
        "        # 피드백을 콘솔에 출력\n",
        "        print(f\"Segment {segment_number}, Frame {frame_number} ({timestamp})\")\n",
        "        print(\"피드백:\")\n",
        "        print(feedback)\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # 이미지 표시\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.imshow(frame)\n",
        "        plt.title(f\"Segment {segment_number}, Frame {frame_number} ({timestamp})\", fontsize=12)\n",
        "        plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "468x2aL_nUDG"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    video_path = \"/content/drive/MyDrive/kakaotech-14-ai/test_video2.mp4\"  # 비디오 파일의 경로\n",
        "    segment_length = 60  # 각 세그먼트의 길이 (초 단위)\n",
        "    frame_interval = 3   # 프레임 추출 간격 (초 단위)\n",
        "\n",
        "    # 비디오의 전체 길이(초 단위)를 가져옵니다.\n",
        "    video_duration = get_video_duration(video_path)\n",
        "\n",
        "    # 비디오를 세그먼트로 분할하기 위한 세그먼트 수 계산\n",
        "    num_segments = math.ceil(video_duration / segment_length)\n",
        "    print(f\"비디오 전체 길이: {int(video_duration // 60)}분 {int(video_duration % 60)}초\")\n",
        "    print(f\"총 {num_segments}개의 세그먼트로 분할됩니다.\")\n",
        "\n",
        "    all_segments_frames = []  # 모든 세그먼트의 프레임들을 저장할 리스트\n",
        "    all_durations = []        # 각 세그먼트의 지속 시간을 저장할 리스트\n",
        "\n",
        "    # 각 세그먼트별로 프레임을 추출합니다.\n",
        "    for i in range(num_segments):\n",
        "        start_time = i * segment_length  # 현재 세그먼트의 시작 시간 (초 단위)\n",
        "\n",
        "        # 마지막 세그먼트의 지속 시간을 조정합니다.\n",
        "        if i == num_segments - 1:\n",
        "            duration = video_duration - start_time  # 남은 시간을 지속 시간으로 설정\n",
        "        else:\n",
        "            duration = segment_length  # 세그먼트 길이를 지속 시간으로 설정\n",
        "\n",
        "        all_durations.append(duration)  # 세그먼트의 지속 시간을 저장\n",
        "\n",
        "        print(f\"샘플링 중인 세그먼트 {i+1}/{num_segments} (시작 시간: {int(start_time // 60)}분 {int(start_time % 60)}초)\")\n",
        "\n",
        "        # 해당 세그먼트에서 프레임을 추출합니다.\n",
        "        clip = download_and_sample_video_local(\n",
        "            video_path, start_time=start_time, duration=duration, frame_interval=frame_interval\n",
        "        )\n",
        "\n",
        "        # 추출된 프레임이 있으면 리스트에 추가합니다.\n",
        "        if clip is not None and len(clip) > 0:\n",
        "            all_segments_frames.append(clip)\n",
        "        else:\n",
        "            print(f\"세그먼트 {i+1}에서 프레임을 추출할 수 없습니다.\")\n",
        "\n",
        "    print(f\"총 {len(all_segments_frames)}개의 세그먼트가 추출되었습니다.\")\n",
        "\n",
        "    all_problematic_frames = []\n",
        "    all_feedbacks = []\n",
        "\n",
        "    # 모든 세그먼트에 대해 프레임 분석을 수행합니다.\n",
        "    for idx, (segment_frames, duration) in enumerate(zip(all_segments_frames, all_durations)):\n",
        "        print(f\"\\nAnalyzing segment {idx+1}/{len(all_segments_frames)}\")\n",
        "        # 각 세그먼트의 프레임들을 분석합니다.\n",
        "        problematic_frames, feedbacks = analyze_frames(\n",
        "            segment_frames, idx, duration, segment_length, frame_interval=frame_interval\n",
        "        )\n",
        "        all_problematic_frames.extend(problematic_frames)\n",
        "        all_feedbacks.extend(feedbacks)\n",
        "\n",
        "    # 문제 있는 프레임들을 시각화합니다.\n",
        "    plot_problematic_frames(all_problematic_frames, all_feedbacks)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rJ45xE98kZU",
        "outputId": "e6e530fe-b8cc-4563-d138-278ec81fd8fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "비디오 전체 길이: 2분 3초\n",
            "총 3개의 세그먼트로 분할됩니다.\n",
            "샘플링 중인 세그먼트 1/3 (시작 시간: 0분 0초)\n",
            "샘플링 중인 세그먼트 2/3 (시작 시간: 1분 0초)\n",
            "샘플링 중인 세그먼트 3/3 (시작 시간: 2분 0초)\n",
            "총 3개의 세그먼트가 추출되었습니다.\n",
            "\n",
            "Analyzing segment 1/3\n",
            "[디버그] 프레임 1 응답 텍스트: 문제 없음.\n",
            "[디버그] 감지된 문제 행동: []\n",
            "[디버그] PROBLEMATIC_BEHAVIORS 리스트: ['과도한 시선 이동', '불규칙한 시선 분산', '무표정', '과도한 표정 변화', '과도한 손동작', '불필요한 손동작', '구부정한 자세', '과도한 움직임', '예상치 못한 행동', '발표 흐름 방해']\n",
            "[디버그] 프레임 2 응답 텍스트: 죄송합니다. 이미지 데이터를 처리하거나 분석할 수 없습니다. 온라인 발표의 비언어적 행동을 분석하여 피드백을 제공하려면, 발표 영상에 대한 설명이나 주요 장면에 대한 정보를 텍스트로 제공해 주시면 감사하겠습니다. 발표자의 비언어적 행동을 평가하고 필요한 피드백을 제공해드리겠습니다.\n",
            "[디버그] 감지된 문제 행동: []\n",
            "[디버그] PROBLEMATIC_BEHAVIORS 리스트: ['과도한 시선 이동', '불규칙한 시선 분산', '무표정', '과도한 표정 변화', '과도한 손동작', '불필요한 손동작', '구부정한 자세', '과도한 움직임', '예상치 못한 행동', '발표 흐름 방해']\n",
            "[디버그] 프레임 3 응답 텍스트: 죄송하지만 이미지 데이터를 분석하고 평가할 수 없습니다. 첨부하신 이미지나 파일 기반의 분석은 지원되지 않습니다. 대신, 텍스트 기반의 정보를 제공해 주시면 도와드릴 수 있습니다. 발표자의 비언어적 행동과 관련된 상세한 설명이나 텍스트를 입력해 주시면 유용한 피드백을 제공해 드릴 수 있습니다.\n",
            "[디버그] 감지된 문제 행동: []\n",
            "[디버그] PROBLEMATIC_BEHAVIORS 리스트: ['과도한 시선 이동', '불규칙한 시선 분산', '무표정', '과도한 표정 변화', '과도한 손동작', '불필요한 손동작', '구부정한 자세', '과도한 움직임', '예상치 못한 행동', '발표 흐름 방해']\n",
            "[디버그] 프레임 4 응답 텍스트: 이미지 데이터로 인해 비언어적 행동을 분석할 수 없습니다. 이미지 대신, 온라인 발표 영상의 URL이나 직접 볼 수 있는 형식으로 제공해 주시면 더욱 정확한 피드백을 제공할 수 있습니다.\n",
            "[디버그] 감지된 문제 행동: []\n",
            "[디버그] PROBLEMATIC_BEHAVIORS 리스트: ['과도한 시선 이동', '불규칙한 시선 분산', '무표정', '과도한 표정 변화', '과도한 손동작', '불필요한 손동작', '구부정한 자세', '과도한 움직임', '예상치 못한 행동', '발표 흐름 방해']\n",
            "[디버그] 프레임 5 응답 텍스트: 문제가 식별되지 않은 이미지이므로, '문제 없음'이라고 답변 드리겠습니다.\n",
            "[디버그] 감지된 문제 행동: []\n",
            "[디버그] PROBLEMATIC_BEHAVIORS 리스트: ['과도한 시선 이동', '불규칙한 시선 분산', '무표정', '과도한 표정 변화', '과도한 손동작', '불필요한 손동작', '구부정한 자세', '과도한 움직임', '예상치 못한 행동', '발표 흐름 방해']\n",
            "[디버그] 프레임 6 응답 텍스트: 문제 없음\n",
            "[디버그] 감지된 문제 행동: []\n",
            "[디버그] PROBLEMATIC_BEHAVIORS 리스트: ['과도한 시선 이동', '불규칙한 시선 분산', '무표정', '과도한 표정 변화', '과도한 손동작', '불필요한 손동작', '구부정한 자세', '과도한 움직임', '예상치 못한 행동', '발표 흐름 방해']\n",
            "[디버그] 프레임 7 응답 텍스트: 죄송하지만 이미지 데이터를 직접 분석할 수 없으며, 본문 형태의 입력을 통해서만 정보를 처리할 수 있습니다. 발표자의 비언어적 행동에 대한 피드백이 필요하시면 글로 설명해주시면 감사드리겠습니다.\n",
            "[디버그] 감지된 문제 행동: []\n",
            "[디버그] PROBLEMATIC_BEHAVIORS 리스트: ['과도한 시선 이동', '불규칙한 시선 분산', '무표정', '과도한 표정 변화', '과도한 손동작', '불필요한 손동작', '구부정한 자세', '과도한 움직임', '예상치 못한 행동', '발표 흐름 방해']\n",
            "[디버그] 프레임 8 응답 텍스트: 죄송하지만 이미지나 비디오 데이터를 처리하거나 분석할 수 없습니다. 텍스트 설명을 통해 도움을 드릴 수 있다면 말씀해 주세요.\n",
            "[디버그] 감지된 문제 행동: []\n",
            "[디버그] PROBLEMATIC_BEHAVIORS 리스트: ['과도한 시선 이동', '불규칙한 시선 분산', '무표정', '과도한 표정 변화', '과도한 손동작', '불필요한 손동작', '구부정한 자세', '과도한 움직임', '예상치 못한 행동', '발표 흐름 방해']\n",
            "[디버그] 프레임 9 응답 텍스트: 문제 없음.\n",
            "[디버그] 감지된 문제 행동: []\n",
            "[디버그] PROBLEMATIC_BEHAVIORS 리스트: ['과도한 시선 이동', '불규칙한 시선 분산', '무표정', '과도한 표정 변화', '과도한 손동작', '불필요한 손동작', '구부정한 자세', '과도한 움직임', '예상치 못한 행동', '발표 흐름 방해']\n",
            "[디버그] 프레임 10 응답 텍스트: 죄송하지만, 이미지 데이터가 제공되지 않아서 발표자의 비언어적 행동을 분석할 수 없습니다. 이미지 파일을 확인하거나 다른 형태로 데이터를 제공해 주세요.\n",
            "[디버그] 감지된 문제 행동: []\n",
            "[디버그] PROBLEMATIC_BEHAVIORS 리스트: ['과도한 시선 이동', '불규칙한 시선 분산', '무표정', '과도한 표정 변화', '과도한 손동작', '불필요한 손동작', '구부정한 자세', '과도한 움직임', '예상치 못한 행동', '발표 흐름 방해']\n",
            "[디버그] 프레임 11 응답 텍스트: 문제가 되는 행동이 이미지를 통해 분석할 수 없습니다. \n",
            "\n",
            "이미지 데이터에서 발표자의 비언어적 행동을 분석하려면 이미지에 대한 구체적인 설명이나 실시간 영상이 필요합니다. 따라서 구체적인 피드백을 제공하기 위해서는 비언어적 행동의 맥락, 움직임, 시선, 표정 등을 포함한 자세한 설명이 주어져야 합니다.\n",
            "[디버그] 감지된 문제 행동: []\n",
            "[디버그] PROBLEMATIC_BEHAVIORS 리스트: ['과도한 시선 이동', '불규칙한 시선 분산', '무표정', '과도한 표정 변화', '과도한 손동작', '불필요한 손동작', '구부정한 자세', '과도한 움직임', '예상치 못한 행동', '발표 흐름 방해']\n",
            "[디버그] 프레임 12 응답 텍스트: 이미지 데이터만으로 발표자의 비언어적 행동을 분석하기 어렵습니다. 올바른 분석을 위해서는 이미지 내용에 대한 설명이나 정보를 제공해 주시기 바랍니다. 가능하다면 발표 내용의 요약이나 관련 정보를 텍스트로 제출해주세요.\n",
            "[디버그] 감지된 문제 행동: []\n",
            "[디버그] PROBLEMATIC_BEHAVIORS 리스트: ['과도한 시선 이동', '불규칙한 시선 분산', '무표정', '과도한 표정 변화', '과도한 손동작', '불필요한 손동작', '구부정한 자세', '과도한 움직임', '예상치 못한 행동', '발표 흐름 방해']\n",
            "[디버그] 프레임 13 응답 텍스트: 문제 없음\n",
            "[디버그] 감지된 문제 행동: []\n",
            "[디버그] PROBLEMATIC_BEHAVIORS 리스트: ['과도한 시선 이동', '불규칙한 시선 분산', '무표정', '과도한 표정 변화', '과도한 손동작', '불필요한 손동작', '구부정한 자세', '과도한 움직임', '예상치 못한 행동', '발표 흐름 방해']\n",
            "[디버그] 프레임 14 응답 텍스트: 죄송합니다. 이미지 데이터를 직접 분석할 수 없습니다. 그러나 이미지에 대한 텍스트 설명을 제공해 주시면 도움이 될 것입니다. 예를 들어, 발표자의 시선, 표정, 제스처, 자세 등에 대한 설명이 포함된 경우 개선이 필요한 부분에 대해 피드백을 제공할 수 있습니다.\n",
            "[디버그] 감지된 문제 행동: []\n",
            "[디버그] PROBLEMATIC_BEHAVIORS 리스트: ['과도한 시선 이동', '불규칙한 시선 분산', '무표정', '과도한 표정 변화', '과도한 손동작', '불필요한 손동작', '구부정한 자세', '과도한 움직임', '예상치 못한 행동', '발표 흐름 방해']\n",
            "[디버그] 프레임 15 응답 텍스트: 문제가 발생한 이미지를 분석할 수 없습니다. 비언어적 행동 분석은 이미지가 아닌 동영상 자료를 통해 수행해야 합니다. 비디오 파일이나 발표 영상에 대한 구체적인 설명이 있다면, 비언어적 행동 분석을 도와드릴 수 있습니다.\n",
            "[디버그] 감지된 문제 행동: []\n",
            "[디버그] PROBLEMATIC_BEHAVIORS 리스트: ['과도한 시선 이동', '불규칙한 시선 분산', '무표정', '과도한 표정 변화', '과도한 손동작', '불필요한 손동작', '구부정한 자세', '과도한 움직임', '예상치 못한 행동', '발표 흐름 방해']\n",
            "[디버그] 프레임 16 응답 텍스트: 시선 처리: 문제 없음  \n",
            "얼굴 표정: 문제 없음  \n",
            "제스처 및 손동작: 문제 없음  \n",
            "자세 및 신체 언어: 문제 없음  \n",
            "갑작스러운 행동 및 움직임: 문제 없음\n",
            "[디버그] 감지된 문제 행동: []\n",
            "[디버그] PROBLEMATIC_BEHAVIORS 리스트: ['과도한 시선 이동', '불규칙한 시선 분산', '무표정', '과도한 표정 변화', '과도한 손동작', '불필요한 손동작', '구부정한 자세', '과도한 움직임', '예상치 못한 행동', '발표 흐름 방해']\n",
            "[디버그] 프레임 17 응답 텍스트: 죄송하지만, 이미지 데이터를 처리하거나 분석할 수 없습니다. 대신 온라인 발표의 비언어적 행동에 관련된 질문에 대해 도움을 드릴 수 있습니다. 피드백이 필요한 구체적인 정보를 제공해 주시면 감사하겠습니다.\n",
            "[디버그] 감지된 문제 행동: []\n",
            "[디버그] PROBLEMATIC_BEHAVIORS 리스트: ['과도한 시선 이동', '불규칙한 시선 분산', '무표정', '과도한 표정 변화', '과도한 손동작', '불필요한 손동작', '구부정한 자세', '과도한 움직임', '예상치 못한 행동', '발표 흐름 방해']\n",
            "[디버그] 프레임 18 응답 텍스트: 죄송합니다. 이미지 데이터를 분석할 수 없습니다. 이미지에 등장하는 발표자의 비언어적 행동을 텍스트로 서술해 주시면 분석과 피드백을 제공해 드리도록 하겠습니다.\n",
            "[디버그] 감지된 문제 행동: []\n",
            "[디버그] PROBLEMATIC_BEHAVIORS 리스트: ['과도한 시선 이동', '불규칙한 시선 분산', '무표정', '과도한 표정 변화', '과도한 손동작', '불필요한 손동작', '구부정한 자세', '과도한 움직임', '예상치 못한 행동', '발표 흐름 방해']\n",
            "[디버그] 프레임 19 응답 텍스트: 문제 없음\n",
            "[디버그] 감지된 문제 행동: []\n",
            "[디버그] PROBLEMATIC_BEHAVIORS 리스트: ['과도한 시선 이동', '불규칙한 시선 분산', '무표정', '과도한 표정 변화', '과도한 손동작', '불필요한 손동작', '구부정한 자세', '과도한 움직임', '예상치 못한 행동', '발표 흐름 방해']\n",
            "[디버그] 프레임 20 응답 텍스트: 죄송합니다. 이미지 데이터는 처리할 수 없습니다. 대신 텍스트와 같은 다른 형식의 정보를 제공해 주시면 비언어적 행동에 대한 도움을 드릴 수 있습니다. \n",
            "[디버그] 감지된 문제 행동: []\n",
            "[디버그] PROBLEMATIC_BEHAVIORS 리스트: ['과도한 시선 이동', '불규칙한 시선 분산', '무표정', '과도한 표정 변화', '과도한 손동작', '불필요한 손동작', '구부정한 자세', '과도한 움직임', '예상치 못한 행동', '발표 흐름 방해']\n",
            "\n",
            "Analyzing segment 2/3\n",
            "[디버그] 프레임 1 응답 텍스트: 죄송합니다. 이미지 데이터를 분석할 수 없습니다. 만약 비디오 또는 이미지 파일을 통해 분석을 원하는 경우, 그 파일을 적절한 형식으로 제공해 주시거나 설명을 텍스트로 입력해 주세요.\n",
            "[디버그] 감지된 문제 행동: []\n",
            "[디버그] PROBLEMATIC_BEHAVIORS 리스트: ['과도한 시선 이동', '불규칙한 시선 분산', '무표정', '과도한 표정 변화', '과도한 손동작', '불필요한 손동작', '구부정한 자세', '과도한 움직임', '예상치 못한 행동', '발표 흐름 방해']\n",
            "[디버그] 프레임 2 응답 텍스트: 죄송하지만 이미지 데이터를 분석할 수 없습니다. 비언어적 행동에 대한 분석을 제공하려면 다른 형태의 입력이 필요합니다. 텍스트나 설명을 통해 추가 정보를 제공해 주시면 감사하겠습니다.\n",
            "[디버그] 감지된 문제 행동: []\n",
            "[디버그] PROBLEMATIC_BEHAVIORS 리스트: ['과도한 시선 이동', '불규칙한 시선 분산', '무표정', '과도한 표정 변화', '과도한 손동작', '불필요한 손동작', '구부정한 자세', '과도한 움직임', '예상치 못한 행동', '발표 흐름 방해']\n",
            "[디버그] 프레임 3 응답 텍스트: 죄송합니다. 이미지를 직접 분석할 수 없습니다. 이미지 분석과 관련하여 구체적인 설명을 해주시면 비언어적 행동에 대한 피드백을 제공드리겠습니다.\n",
            "[디버그] 감지된 문제 행동: []\n",
            "[디버그] PROBLEMATIC_BEHAVIORS 리스트: ['과도한 시선 이동', '불규칙한 시선 분산', '무표정', '과도한 표정 변화', '과도한 손동작', '불필요한 손동작', '구부정한 자세', '과도한 움직임', '예상치 못한 행동', '발표 흐름 방해']\n",
            "[디버그] 프레임 4 응답 텍스트: 이미지를 분석할 수 없어 '문제 없음'이라고 답변할 수 없습니다. 이미지 데이터를 제공받을 수 있는 다른 방법을 시도해주세요.\n",
            "[디버그] 감지된 문제 행동: []\n",
            "[디버그] PROBLEMATIC_BEHAVIORS 리스트: ['과도한 시선 이동', '불규칙한 시선 분산', '무표정', '과도한 표정 변화', '과도한 손동작', '불필요한 손동작', '구부정한 자세', '과도한 움직임', '예상치 못한 행동', '발표 흐름 방해']\n",
            "[디버그] 프레임 5 응답 텍스트: 이미지를 분석할 수 없습니다. 텍스트 데이터나 설명을 제공해주시면 분석하여 피드백을 드릴 수 있습니다.\n",
            "[디버그] 감지된 문제 행동: []\n",
            "[디버그] PROBLEMATIC_BEHAVIORS 리스트: ['과도한 시선 이동', '불규칙한 시선 분산', '무표정', '과도한 표정 변화', '과도한 손동작', '불필요한 손동작', '구부정한 자세', '과도한 움직임', '예상치 못한 행동', '발표 흐름 방해']\n",
            "[디버그] 프레임 6 응답 텍스트: 이미지 데이터만으로 발표자의 비언어적 행동을 분석하기 어렵습니다. 입력하신 이미지를 설명하거나 대체 텍스트를 제공하시면 추가적인 피드백을 제공하는 데 도움이 될 수 있습니다. 온라인 발표의 비언어적 커뮤니케이션에 대한 구체적인 피드백을 원하시면 해당 발표의 비디오나 적절한 설명을 사용해 주세요.\n",
            "[디버그] 감지된 문제 행동: []\n",
            "[디버그] PROBLEMATIC_BEHAVIORS 리스트: ['과도한 시선 이동', '불규칙한 시선 분산', '무표정', '과도한 표정 변화', '과도한 손동작', '불필요한 손동작', '구부정한 자세', '과도한 움직임', '예상치 못한 행동', '발표 흐름 방해']\n",
            "[디버그] 프레임 7 응답 텍스트: I'm sorry, but I can't analyze or evaluate visual content such as images directly. If you have a description of the non-verbal behavior exhibited in an online presentation, I can certainly help provide feedback based on that information. Please feel free to describe the content or behaviors you're observing, and I'd be happy to assist you!\n",
            "[디버그] 감지된 문제 행동: []\n",
            "[디버그] PROBLEMATIC_BEHAVIORS 리스트: ['과도한 시선 이동', '불규칙한 시선 분산', '무표정', '과도한 표정 변화', '과도한 손동작', '불필요한 손동작', '구부정한 자세', '과도한 움직임', '예상치 못한 행동', '발표 흐름 방해']\n",
            "[디버그] 프레임 8 응답 텍스트: 죄송합니다. 이미지 데이터를 분석할 수 있는 기능이 없습니다. 비언어적 행동 피드백을 제공하려면 구체적인 행동이나 상황에 대한 서술적 정보가 필요합니다.\n",
            "[디버그] 감지된 문제 행동: []\n",
            "[디버그] PROBLEMATIC_BEHAVIORS 리스트: ['과도한 시선 이동', '불규칙한 시선 분산', '무표정', '과도한 표정 변화', '과도한 손동작', '불필요한 손동작', '구부정한 자세', '과도한 움직임', '예상치 못한 행동', '발표 흐름 방해']\n",
            "[디버그] 프레임 9 응답 텍스트: 이미지만으로는 비언어적 행동을 평가할 수 없습니다. 대신 명확한 피드백을 제공하려면 영상이나 설명이 필요합니다. 이미지 데이터 대신 관련된 설명을 제공해주시면 도움이 될 것입니다.\n",
            "[디버그] 감지된 문제 행동: []\n",
            "[디버그] PROBLEMATIC_BEHAVIORS 리스트: ['과도한 시선 이동', '불규칙한 시선 분산', '무표정', '과도한 표정 변화', '과도한 손동작', '불필요한 손동작', '구부정한 자세', '과도한 움직임', '예상치 못한 행동', '발표 흐름 방해']\n",
            "[디버그] 프레임 10 응답 텍스트: 이미지 분석 결과: 'data:image/jpeg;base64,/9j/...' 대한 문제를 판단할 수 없습니다. 이미지를 해석할 수 있는 기능이 없습니다. 적절한 피드백을 제공하기 위해서는 영상이나 피드백이 필요한 구체적인 행동에 대한 설명이 필요합니다.\n",
            "[디버그] 감지된 문제 행동: []\n",
            "[디버그] PROBLEMATIC_BEHAVIORS 리스트: ['과도한 시선 이동', '불규칙한 시선 분산', '무표정', '과도한 표정 변화', '과도한 손동작', '불필요한 손동작', '구부정한 자세', '과도한 움직임', '예상치 못한 행동', '발표 흐름 방해']\n",
            "[디버그] 프레임 11 응답 텍스트: 문제 없음\n",
            "[디버그] 감지된 문제 행동: []\n",
            "[디버그] PROBLEMATIC_BEHAVIORS 리스트: ['과도한 시선 이동', '불규칙한 시선 분산', '무표정', '과도한 표정 변화', '과도한 손동작', '불필요한 손동작', '구부정한 자세', '과도한 움직임', '예상치 못한 행동', '발표 흐름 방해']\n",
            "[디버그] 프레임 12 응답 텍스트: 문제 없음\n",
            "[디버그] 감지된 문제 행동: []\n",
            "[디버그] PROBLEMATIC_BEHAVIORS 리스트: ['과도한 시선 이동', '불규칙한 시선 분산', '무표정', '과도한 표정 변화', '과도한 손동작', '불필요한 손동작', '구부정한 자세', '과도한 움직임', '예상치 못한 행동', '발표 흐름 방해']\n",
            "[디버그] 프레임 13 응답 텍스트: 문제로 분석할 수 있는 행동이 없는 이미지이므로, '문제 없음'이라고 판단됩니다.\n",
            "[디버그] 감지된 문제 행동: []\n",
            "[디버그] PROBLEMATIC_BEHAVIORS 리스트: ['과도한 시선 이동', '불규칙한 시선 분산', '무표정', '과도한 표정 변화', '과도한 손동작', '불필요한 손동작', '구부정한 자세', '과도한 움직임', '예상치 못한 행동', '발표 흐름 방해']\n",
            "[디버그] 프레임 14 응답 텍스트: 문제 없음\n",
            "[디버그] 감지된 문제 행동: []\n",
            "[디버그] PROBLEMATIC_BEHAVIORS 리스트: ['과도한 시선 이동', '불규칙한 시선 분산', '무표정', '과도한 표정 변화', '과도한 손동작', '불필요한 손동작', '구부정한 자세', '과도한 움직임', '예상치 못한 행동', '발표 흐름 방해']\n",
            "[디버그] 프레임 15 응답 텍스트: 죄송하지만 이미지 데이터를 이해하거나 처리할 수 없습니다. 온라인 발표의 비언어적 행동에 대해 서면으로 설명해 주시면 그에 대한 피드백을 제공할 수 있습니다.\n",
            "[디버그] 감지된 문제 행동: []\n",
            "[디버그] PROBLEMATIC_BEHAVIORS 리스트: ['과도한 시선 이동', '불규칙한 시선 분산', '무표정', '과도한 표정 변화', '과도한 손동작', '불필요한 손동작', '구부정한 자세', '과도한 움직임', '예상치 못한 행동', '발표 흐름 방해']\n",
            "[디버그] 프레임 16 응답 텍스트: 죄송합니다, 이미지를 분석할 수 없습니다. 대신 설명할 수 있는 다른 정보가 있으면 제공해 주시면 감사하겠습니다.\n",
            "[디버그] 감지된 문제 행동: []\n",
            "[디버그] PROBLEMATIC_BEHAVIORS 리스트: ['과도한 시선 이동', '불규칙한 시선 분산', '무표정', '과도한 표정 변화', '과도한 손동작', '불필요한 손동작', '구부정한 자세', '과도한 움직임', '예상치 못한 행동', '발표 흐름 방해']\n",
            "[디버그] 프레임 17 응답 텍스트: 죄송합니다. 이미지를 분석하거나 시각적 콘텐츠를 처리할 수 없습니다. 비언어적 행동 분석에 대한 피드백이 필요한 경우, 이미지 대신 발표에 대한 서술적 정보를 제공해 주시면 감사하겠습니다.\n",
            "[디버그] 감지된 문제 행동: []\n",
            "[디버그] PROBLEMATIC_BEHAVIORS 리스트: ['과도한 시선 이동', '불규칙한 시선 분산', '무표정', '과도한 표정 변화', '과도한 손동작', '불필요한 손동작', '구부정한 자세', '과도한 움직임', '예상치 못한 행동', '발표 흐름 방해']\n",
            "[디버그] 프레임 18 응답 텍스트: 죄송하지만, 이미지 데이터를 분석할 수 없습니다. 텍스트로 설명된 발표 상황이나 비언어적 행동에 대한 묘사가 있다면, 이를 토대로 피드백을 제공해 드릴 수 있습니다. 만약 구체적인 설명이나 다른 형식의 데이터를 제공할 수 있다면, 다시 시도해 주세요.\n",
            "[디버그] 감지된 문제 행동: []\n",
            "[디버그] PROBLEMATIC_BEHAVIORS 리스트: ['과도한 시선 이동', '불규칙한 시선 분산', '무표정', '과도한 표정 변화', '과도한 손동작', '불필요한 손동작', '구부정한 자세', '과도한 움직임', '예상치 못한 행동', '발표 흐름 방해']\n",
            "[디버그] 프레임 19 응답 텍스트: 문제 없음\n",
            "[디버그] 감지된 문제 행동: []\n",
            "[디버그] PROBLEMATIC_BEHAVIORS 리스트: ['과도한 시선 이동', '불규칙한 시선 분산', '무표정', '과도한 표정 변화', '과도한 손동작', '불필요한 손동작', '구부정한 자세', '과도한 움직임', '예상치 못한 행동', '발표 흐름 방해']\n",
            "[디버그] 프레임 20 응답 텍스트: 죄송합니다만, 주어진 이미지 데이터는 분석할 수 없습니다. 텍스트 설명이나 이미지 설명을 제공해주시면 비언어적 행동에 대한 피드백을 제공해드리도록 하겠습니다.\n",
            "[디버그] 감지된 문제 행동: []\n",
            "[디버그] PROBLEMATIC_BEHAVIORS 리스트: ['과도한 시선 이동', '불규칙한 시선 분산', '무표정', '과도한 표정 변화', '과도한 손동작', '불필요한 손동작', '구부정한 자세', '과도한 움직임', '예상치 못한 행동', '발표 흐름 방해']\n",
            "\n",
            "Analyzing segment 3/3\n",
            "[디버그] 프레임 1 응답 텍스트: 문제 없음\n",
            "[디버그] 감지된 문제 행동: []\n",
            "[디버그] PROBLEMATIC_BEHAVIORS 리스트: ['과도한 시선 이동', '불규칙한 시선 분산', '무표정', '과도한 표정 변화', '과도한 손동작', '불필요한 손동작', '구부정한 자세', '과도한 움직임', '예상치 못한 행동', '발표 흐름 방해']\n",
            "[디버그] 프레임 2 응답 텍스트: 죄송하지만 주어진 이미지를 분석하지 못합니다. 이미지 데이터를 텍스트나 설문으로 설명해 주시면, 이에 따라 적절한 피드백을 제공할 수 있습니다.\n",
            "[디버그] 감지된 문제 행동: []\n",
            "[디버그] PROBLEMATIC_BEHAVIORS 리스트: ['과도한 시선 이동', '불규칙한 시선 분산', '무표정', '과도한 표정 변화', '과도한 손동작', '불필요한 손동작', '구부정한 자세', '과도한 움직임', '예상치 못한 행동', '발표 흐름 방해']\n",
            "문제 있는 프레임이 없습니다.\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbNmwDRz-1e7"
      },
      "source": [
        "ver1. 해결문제\n",
        "- 정확도 이슈 (일단 정확하게 올려야 할지도 -> 파인튜닝 필요..?)\n",
        "- 프레임 간격 더 줄이기 (3초 -> 1초)\n",
        "- webm 형태로 제공? 이건 추후 의논 필요."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
